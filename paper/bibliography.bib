@article{Friedman2000,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2674028},
 abstract = {Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.},
 author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
 journal = {The Annals of Statistics},
 number = {2},
 pages = {337--374},
 publisher = {Institute of Mathematical Statistics},
 title = {Special Invited Paper. Additive Logistic Regression: A Statistical View of Boosting},
 urldate = {2023-08-26},
 volume = {28},
 year = {2000}
}

@article{Armijo1966,
author = {Larry Armijo},
title = {{Minimization of functions having Lipschitz continuous first partial derivatives.}},
volume = {16},
journal = {Pacific Journal of Mathematics},
number = {1},
publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
pages = {1 -- 3},
year = {1966},
}

@article{montiel2021river,
  title={River: machine learning for streaming data in Python},
  author={Montiel, Jacob and Halford, Max and Mastelini, Saulo Martiello
          and Bolmier, Geoffrey and Sourty, Raphael and Vaysse, Robin and Zouitine, Adil
          and Gomes, Heitor Murilo and Read, Jesse and Abdessalem, Talel and others},
  year={2021}
}


@Book{wickham2015,
 author = {Wickham, Hadley},
 title = {R Packages},
 year = {2015},
 isbn = {1491910593, 9781491910597},
 edition = {1st},
 publisher = {O'Reilly Media, Inc.},
}
@article{knuth1984,
  title={Literate programming},
  author={Knuth, Donald E.},
  journal={The Computer Journal},
  volume={27},
  number={2},
  pages={97--111},
  year={1984},
  publisher={British Computer Society}
}

@article{MOA2010,
author = {Bifet, Albert and Holmes, Geoff and Kirkby, Richard and Pfahringer, Bernhard},
title = {MOA: Massive Online Analysis},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Naive Bayes classifiers at the leaves. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis, and is released under the GNU GPL license.},
journal = {J. Mach. Learn. Res.},
month = {aug},
pages = {1601–1604},
numpages = {4}
}

@inproceedings{hulten2001,
author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
title = {Mining Time-Changing Data Streams},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502529},
doi = {10.1145/502512.502529},
abstract = {Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {97–106},
numpages = {10},
keywords = {data streams, subsampling, Decision trees, concept drift, incremental learning, Hoeffding bounds},
location = {San Francisco, California},
series = {KDD '01}
}


@InProceedings{oza01a,
  title = 	 {Online Bagging and Boosting},
  author =       {Oza, Nikunj C. and Russell, Stuart J.},
  booktitle = 	 {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},
  pages = 	 {229--236},
  year = 	 {2001},
  editor = 	 {Richardson, Thomas S. and Jaakkola, Tommi S.},
  volume = 	 {R3},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {04--07 Jan},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/r3/oza01a/oza01a.pdf},
  url = 	 {https://proceedings.mlr.press/r3/oza01a.html},
  abstract = 	 {Bagging and boosting are well-known ensemble learning methods. They combine multiple learned base models with the aim of improving generalization performance. To date, they have been used primarily in batch mode, and no effective online versions have been proposed. We present simple online bagging and boosting algorithms that we claim perform as well as their batch counterparts.},
  note =         {Reissued by PMLR on 31 March 2021.}
}

@inproceedings{NIPS2000_d072677d,
 author = {Gentile, Claudio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {A New Approximate Maximal Margin Classification Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf},
 volume = {13},
 year = {2000}
}


@article{gomes2017adaptive,
  title={Adaptive random forests for evolving data stream classification},
  author={Gomes, Heitor M and Bifet, Albert and Read, Jesse and Barddal, Jean Paul and Enembreck, Fabricio and Pfharinger, Bernhard and Holmes, Geoff and Abdessalem, Talel},
  journal={Machine Learning},
  volume={106},
  pages={1469--1495},
  year={2017},
  publisher={Springer}
}

@article{mourtada2019amf,
  title={Amf: Aggregated mondrian forests for online learning},
  author={Mourtada, Jaouad and Gaiffas, Stephane and Scornet, Erwan},
  journal={arXiv preprint arXiv:1906.10529},
  year={2019}
}
